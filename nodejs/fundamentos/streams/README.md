# ğŸ“˜ Streams em Node.js - Fundamentos

Bem-vindo ao repositÃ³rio dedicado ao estudo de **Streams** em **Node.js**, localizado na pasta `/nodejs/fundamentos/streams` do meu **Caderno de Estudos**. Este projeto foi criado para solidificar meus conhecimentos sobre o uso de streams, uma ferramenta essencial para processar grandes volumes de dados de forma eficiente, evitando travamentos e otimizando o desempenho de aplicaÃ§Ãµes.

A ideia aqui Ã© explorar os conceitos bÃ¡sicos e prÃ¡ticos das streams, incluindo **Readable Streams**, **Transform Streams**, e **Writable Streams**, com exemplos funcionais que demonstram como lidar com grandes arquivos e processamentos complexos.

---

## ğŸ¯ Objetivo

O objetivo deste repositÃ³rio Ã© entender como as streams no Node.js podem ser usadas para ler, transformar e escrever dados em pedaÃ§os (chunks), especialmente em cenÃ¡rios onde grandes quantidades de dados (como arquivos de 2 GB ou mais) poderiam quebrar uma aplicaÃ§Ã£o se processadas de uma sÃ³ vez. Inclui exemplos prÃ¡ticos e analogias para facilitar o aprendizado.

---

## ğŸ’¡ O que sÃ£o Streams?

Grandes cargas de memÃ³ria de uma sÃ³ vez em Node.js podem travar a aplicaÃ§Ã£o. Tentar ler um arquivo muito grande, como 2 GB, por exemplo, Ã© um caso clÃ¡ssico de uso onde streams se tornam indispensÃ¡veis. Streams permitem ler uma carga massiva de dados, processÃ¡-los por partes e devolver o resultado apenas quando todo o processamento estiver concluÃ­do.

No Node.js, ao receber um arquivo, ele Ã© transformado em **Buffers**, que sÃ£o divididos em pedaÃ§os chamados **Chunks**. Esses chunks sÃ£o processados sequencialmente, semelhante a um fluxo contÃ­nuo.

### Analogia com o Mundo Real

Imagine uma pepita de ouro bruta de 10 GB, representando nosso arquivo bruto. Para processÃ¡-la:
- **Derreter a pepita** equivale a transformÃ¡-la em **Buffers/Chunks**.
- **Jogar em um funil** representa o uso de uma **Readable Stream**, que fornece os dados para o processamento.
- **Filtrar e adicionar produtos quÃ­micos** (remover impurezas, transformar formatos) Ã© o papel das **Transform Streams**.
- **Transformar em pequenas pepitas e retirÃ¡-las** Ã© a funÃ§Ã£o das **Writable Streams**, que entregam o produto final (ex: console, arquivo, banco de dados).

Assim, em vez de jogar uma tonelada de ouro no funil de uma vez, diluÃ­mos em formato lÃ­quido (stream) e processamos sob demanda. Esse "funil" em Node.js Ã© chamado de **Pipelines**.

### Tipos de Streams

- **Readable Stream**: Fonte de dados, como um arquivo, banco de dados, requisiÃ§Ã£o web ou qualquer entrada de dados.
- **Transform Stream**: Etapa de conversÃ£o, onde os chunks sÃ£o mapeados, filtrados ou transformados antes de seguir adiante.
- **Writable Stream**: Etapa final, que recebe cada chunk e o envia para uma saÃ­da, como console.log, arquivo ou cliente.

### ObservaÃ§Ãµes
- Web APIs e o mÃ³dulo nativo HTTP do Node.js utilizam streams internamente. Por exemplo, uma requisiÃ§Ã£o (`request`) Ã© uma **Readable Stream**, e a resposta (`response`) Ã© uma **Writable Stream**.
- Casos de uso comuns incluem leitura/escrita de arquivos grandes, relatÃ³rios, extraÃ§Ã£o/compressÃ£o de arquivos, processamento de Ã¡udio/vÃ­deo e processos ETL.

---

## ğŸš€ Exemplos PrÃ¡ticos

Este repositÃ³rio contÃ©m exemplos prÃ¡ticos que demonstram o uso de streams em diferentes cenÃ¡rios.

### Estrutura do CÃ³digo

#### Exemplo 01 (`example01.js`)
Este arquivo apresenta casos bÃ¡sicos de streams com diferentes abordagens:

- **Exemplo 01 - `stdin` e `stdout`**:
  - LÃª dados do terminal (`process.stdin`) e escreve no terminal (`process.stdout`).
  - Uso: `node -e "process.stdin.on('data', msg => process.stdout.write(msg.toString()))"`.

- **Exemplo 02 - HTTP com Streams**:
  - Cria um servidor HTTP que lÃª um arquivo grande (`big.file`, gerado com `node -e "process.stdout.write(crypto.randomBytes(1e9))" > big.file`) e o envia como resposta usando `pipe`.
  - Uso: `node example01.js` e acesse `http://localhost:3000`.

- **Exemplo 03 - Sockets**:
  - Utiliza `net` para criar um servidor que recebe dados via socket e os redireciona para `stdout`.
  - Uso: Execute `node example01.js` e envie dados com `node -e "process.stdin.pipe(require('net').connect(1338))"`.

- **Dica**: Procure por `.on("error")`, `.on("close")` ou outros `.on()` para identificar streams. A melhor abordagem Ã© usar **Readable**, **Writable**, e **Transform Streams**.

#### Exemplo 02 (`example02.js`)
Este arquivo mostra um exemplo mais avanÃ§ado, utilizando pipelines para processar dados:

- **Processo 01**:
  - Cria uma **Readable Stream** que emite mensagens ("Hello Dude!!0", "Hello Dude!!1", "Hello Dude!!2") e as envia para uma **Writable Stream** que as exibe no console.
  - Uso: Demonstra o fluxo bÃ¡sico de dados com `pipelineAsync`.

- **Processo 02**:
  - Gera 100.000 objetos (`person`) em uma **Readable Stream**.
  - Usa uma **Transform Stream** (`writableMapToCSV`) para converter cada objeto em formato CSV (ID, Nome em maiÃºsculo).
  - Usa outra **Transform Stream** (`setHeader`) para adicionar um cabeÃ§alho ao CSV na primeira linha.
  - Salva o resultado em um arquivo `my.csv` com uma **Writable Stream** (`createWriteStream`).
  - Uso: Gera um arquivo CSV com dados processados em partes.

### CÃ³digo Resumido

#### `example01.js` (Trecho)
```javascript
import net from "node:net";
net.createServer((socket) => socket.pipe(process.stdout)).listen(1338);
```

#### `example02.js` (Trecho)
```javascript
const pipelineAsync = promisify(pipeline);

const readableStream = Readable({
  read() {
    for (let index = 0; index < 1e5; index++) {
      const person = { id: Date.now() + index, name: `Richard-${index}` };
      this.push(JSON.stringify(person));
    }
    this.push(null);
  },
});

await pipelineAsync(
  readableStream,
  writableMapToCSV,
  setHeader,
  createWriteStream("my.csv")
);
```

---

## ğŸ›  Tecnologias Utilizadas

- **Node.js**: Ambiente de execuÃ§Ã£o para os exemplos.
- **Streams**: MÃ³dulo nativo para leitura, transformaÃ§Ã£o e escrita de dados.

---

## ğŸ§ª Como Rodar o Projeto

### PrÃ©-requisitos
- **Node.js** (versÃ£o 16 ou superior)

### Passos

1. **Clone o repositÃ³rio**:
   ```bash
   git clone https://github.com/seu-usuario/caderno.git
   cd caderno/nodejs/fundamentos/streams
   ```

2. **Crie um arquivo grande para testes**:
   ```bash
   node -e "process.stdout.write(crypto.randomBytes(1e9))" > big.file
   ```
   - Isso gera um arquivo de quase 1 GB chamado `big.file`.

3. **Execute os exemplos**:
   - Para `example01.js`:
     - `node example01.js` e acesse `http://localhost:3000` para o exemplo HTTP, ou use sockets com `node -e "process.stdin.pipe(require('net').connect(1338))"`.
   - Para `example02.js`:
     - `node example02.js` para gerar o arquivo `my.csv`.

---

## ğŸ“ Estrutura do Projeto

```plaintext
streams/
â”œâ”€â”€ example01.js
â”œâ”€â”€ example02.js
â””â”€â”€ README.md
```

---

## ğŸ“Œ Conceitos Demonstrados

- **Streams BÃ¡sicas**: Uso de `Readable`, `Writable`, e `Transform Streams` com exemplos simples.
- **Pipelines**: Encadeamento de streams com `pipeline` para processar dados em etapas.
- **Processamento de Arquivos Grandes**: Leitura e escrita de dados em chunks para evitar travamentos.
- **HTTP e Sockets**: AplicaÃ§Ã£o de streams em requisiÃ§Ãµes web e comunicaÃ§Ã£o via socket.
- **TransformaÃ§Ã£o de Dados**: ConversÃ£o de objetos JSON para CSV com controle de cabeÃ§alho.

---

## ğŸ” ConsideraÃ§Ãµes Adicionais

- **EficiÃªncia**: Streams sÃ£o ideais para cenÃ¡rios onde a memÃ³ria Ã© um limite, como relatÃ³rios ou arquivos grandes.
- **Escalabilidade**: O uso de pipelines permite processar dados em tempo real, mesmo em grandes volumes.
- **Extensibilidade**: A lÃ³gica pode ser adaptada para integraÃ§Ãµes com bancos de dados, APIs ou compressÃ£o de arquivos.

Este repositÃ³rio Ã© uma base prÃ¡tica para entender e aplicar streams em Node.js, com foco em fundamentos e casos reais de uso.